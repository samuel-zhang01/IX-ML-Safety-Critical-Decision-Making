{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtFDX-XeI28u"
      },
      "source": [
        "# Homework #1 (Due 09/02/2026, 11:59pm)\n",
        "\n",
        "**ELEC70122: ML for Safety Critical Decision-Making**<br>\n",
        "**Instructor: Sonali Parbhoo**<br>\n",
        "**Fall 2026**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv2cSqqnI28u"
      },
      "source": [
        "**Name:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0blSqJAMI28v"
      },
      "source": [
        "### Instructions:\n",
        "\n",
        "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
        "\n",
        "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. ***Portions of your submission that contains syntactic or run-time errors will not be graded***.\n",
        "\n",
        "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fEqXx4rTI28v"
      },
      "outputs": [],
      "source": [
        "### Import basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L81aOLbtI28w"
      },
      "outputs": [],
      "source": [
        "def get_posterior_samples(prior_var, noise_var, x_matrix, y_matrix, x_test_matrix, samples=100):\n",
        "    '''Function to generate posterior predictive samples for Bayesian linear regression model'''\n",
        "    prior_variance = np.diag(prior_var * np.ones(x_matrix.shape[1]))\n",
        "    prior_precision = np.linalg.inv(prior_variance)\n",
        "\n",
        "    joint_precision = prior_precision + x_matrix.T.dot(x_matrix) / noise_var\n",
        "    joint_variance = np.linalg.inv(joint_precision)\n",
        "    joint_mean = joint_variance.dot(x_matrix.T.dot(y_matrix)) / noise_var\n",
        "\n",
        "    #sampling 100 points from the posterior\n",
        "    posterior_samples = np.random.multivariate_normal(joint_mean.flatten(), joint_variance, size=samples)\n",
        "\n",
        "    #take posterior predictive samples\n",
        "    posterior_predictions = np.dot(posterior_samples, x_test_matrix.T)\n",
        "    posterior_predictive_samples = posterior_predictions[np.newaxis, :, :] + np.random.normal(0, noise_var**0.5, size=(100, posterior_predictions.shape[0], posterior_predictions.shape[1]))\n",
        "    posterior_predictive_samples = posterior_predictive_samples.reshape((100 * posterior_predictions.shape[0], posterior_predictions.shape[1]))\n",
        "    return posterior_predictions, posterior_predictive_samples\n",
        "\n",
        "\n",
        "def generate_data(number_of_points=10, noise_variance=0.3):\n",
        "    '''Function for generating toy regression data'''\n",
        "    #training x\n",
        "    x_train = np.hstack((np.linspace(-1, -0.5, number_of_points), np.linspace(0.5, 1, number_of_points)))\n",
        "    #function relating x and y\n",
        "    f = lambda x: 3 * x**3\n",
        "    #y is equal to f(x) plus gaussian noise\n",
        "    y_train = f(x_train) + np.random.normal(0, noise_variance**0.5, 2 * number_of_points)\n",
        "    x_test = np.array(list(set(list(np.hstack((np.linspace(-1, 1, 200), x_train))))))\n",
        "    x_test = np.sort(x_test)\n",
        "    return x_train, y_train, x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPmOtVBI28w"
      },
      "source": [
        "## Part I: Maximum Likelihood Estimators for Polynomial Regression\n",
        "\n",
        "In this problem, you are given a function, `generate_data`, to generate toy datasets with a single predictor $X$ representing patient age (normalized) and a single outcome $y$ representing diastolic blood pressure (normalized and rescaled), and your task is to fit polynomial models to the data. That is, assume that the outcome $y$ can be modeled by the following process:\n",
        "\n",
        "\\begin{align}\n",
        "y &= f(x) + \\epsilon = w_0 + w_1x + w_2x^2 + \\ldots + w_Dx^D + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.3)\n",
        "\\end{align}\n",
        "\n",
        "where the $w_d$, the *parameters* of the function $f$, are unknown constants, and the degree $D$ is a hyperparameter.\n",
        "\n",
        "\n",
        "You'll notice that in these datasets, the test input is sampled from a different distribution that the training input: the training input has a gap, there are no training input values in [-0.5, 0.5], where as the test input are sampled across [-1, 1]. This change of the distributions over the $x$-values between training and test is called **covariate shift**.\n",
        "\n",
        "These toy datasets simulate a very common problem in machine learning: models are fitted on training data, but during deployment they are given data dissimlar to the training data (i.e. the model encounters covariate shift). As such, you should treat `x_train`, `y_train` as data available during model development and evaluation, and `x_test` as data you encounter during model deployment.\n",
        "\n",
        "The goal in this assignment is to explore how to manage the risk of a deployed model under covariate shift. The ideas developed in this assignment will become a major focus in the latter part of the course and the foundation of an active area of current research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXOWB78UI28w"
      },
      "source": [
        "1. **(Effect of Model Complexity)** Generate a toy dataset with 40 observations (set the parameter `number_of_points=20` for `generate_data`, since twice the number of `number_of_points` will be generated), then  visualize the fit of MLE polynomial models, with degrees $D = [1, 3, 5, 10, 15, 20, 50, 100]$ - you should train on `x_train` and **visualize by predicting on `x_test` provided by the data generating function (`x_test` is a larger set of test points that includes `x_train`)**. You'll need to be thoughtful about your visualization so that these different models can be visually compared in a meaningful way. <br><br>\n",
        "Discuss the effect of the choice of polynomial degree on the fit of the model (concretely describe why certaint choices are unideal in the context of the problem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssl6N_HJI28w"
      },
      "source": [
        "2. **(Model Selection)** Later in the course, we will study a number of metrics commonly used for selecting between different MLE models. All of these metric essentially encode Occam's Razor: select the minimal complexity of model that satisfies some pre-determined modeling goal. <br><br>\n",
        "For now, a very simple method for selecting the optimal degree is via cross-validation (by bootstrap):\n",
        "\n",
        "  1. randomly sample two datasets, `x_train`, `x_valid`, from the data generating function: one for training and one for validation. Fit an MLE  polynomial model of degree $d$ on the training data and evaluate its performance on the validation data. Over $S$ number of such randomly sampled pairs of datasets, average the model's validation performance.\n",
        "  2. plot the validation score as a function of model complexity, the polynomial degree $d$.\n",
        "  3. based on the plot, select the the minimal degree that achieves a high average validation performance (i.e. look for the 'elbow' of the plot).\n",
        "\n",
        "  Explain why performing model selection by cross-validation mitigates the risk of choosing an undesirable polynomial (identified in Problem 1)?<br><br>\n",
        "  Implement model selection by cross-validation for the toy dataset generated in Problem 1 using MSE as your performance metric and select an optimal degree from $D=[1,3,5,10,15,20,50,100]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ZWxD4MI28w"
      },
      "source": [
        "3. **(Uncertainty Estimation)** We often use the bootstrap predictive uncertainty of MLE models as an indicator of our confidence in the model's output. Increasingly, in practice, decisions making are deferred to human experts when the model's predictive uncertainty is too high. <br><br>\n",
        "Given your understanding of the dataset (`x_train` and `x_test`), describe what the model uncertainty ***should*** ideally look like across the input space (i.e. if you plotted the model uncertainty as a function of $x$, what would it look like)? Justify your answer: consider the context of the problem - the test input has undergone covariate shift and is dissmilar to the training input, what kind of uncertainty would help you mitigate risk under this condition?<br><br>\n",
        "A common practice for estimating predictive uncertainty is to fit a large number of (bootstrap) models on the training data (this collection of models is called an **ensemble**), then, at an input $x$, use the variance of the ensemble predictions to estimate the uncertainty at $x$. Plot the 95% predictive interval of 200 bootstrap MLE polynomial models for each degree $D=[1,3,5,10,15,20,50,100]$, arrange your plots as  subplots in a single figure. For which polynomial degree do you obtain the predictive uncertainty that is most ideal (according to your description above)? Is this the degree you selected in Problem 2? Explain why you would or would not expect the optimal degree in Problem 2 to yield the most ideal uncertainty estimate.<br><br>\n",
        "Make the same plots the 95% predictive intervals for degrees $D=[1,3,5,10,15,20,50,100]$, with models fitted on larger training datasets - set `number_of_points` to 50, 100, 500, 1000 (arrange all these plots in a single figure). What is happening to the predictions of the ensemble in the training data rich region? What is happening to the predictions of the ensemble in the training data poor region? Are these expected behaviours (relate what you see in both cases to the asymptotic properties of MLE)?\n",
        "<br><br>\n",
        "When the training data is abundant (`number_of_points=1000`), are any of the 95% predictive intervals ideal (according to your description above)? What does this imply about the feasibility of using the variance of the ensemble predictions to estimate predictive uncertainty at an input $x$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpGHl0mCI28x"
      },
      "source": [
        "4. **(Effect of Regularization)** In practice, MLE models are nearly always trained with regularization (since they tend to overfit to the training data). Here, we will explore the effect of adding $\\ell_2$ regularization to our MLE polynomial models (that is, use the `Ridge` regression model from `sklearn` after augmenting your input with polynomial features). <br><br>\n",
        "For a toy dataset with 40 observations (`number_of_points=20`), plot the 95% predictive intervals for degrees $D = [1,3,5,10,15,20,50,100]$ and regularization strengths `alpha = [5e-3, 1e-2, 1e-1, 1e0, 1e1]` (you should organize these plots in a grid).<br><br>\n",
        "Describe the effect of regularization on the bootstrap uncertainties. Looking at these results, are the goals of $\\ell_2$ regularization and obtaining useful predictive uncertatinty estimation neccessarily well-aligned?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjB_7gWCI28x"
      },
      "source": [
        "## Part II: Bayesian Polynomial Regression\n",
        "In this problem, your task is to perform Bayesian polynomial regression on the toy datasets in Part I. That is, assume that the outcome $y$ can be modeled by the following process:\n",
        "\n",
        "\\begin{align}\n",
        "y &= f(x) + \\epsilon = w_0 + w_1x + w_2x^2 + \\ldots + w_Dx^D + \\epsilon, \\quad \\epsilon \\sim N(0, 0.3)\\\\\n",
        "w_d &\\sim N(0, \\alpha)\n",
        "\\end{align}\n",
        "\n",
        "where $\\alpha$ is a hyperparameter and must be fixed before modeling and inference begins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl_qdaNbI28x"
      },
      "source": [
        "1. **(Bayesian Kernel Regression)** Just as we can treat a polynomial regression model like a multi-linear regression model after ***transforming*** the input data by adding polynomial features. We can treat Bayesian polynomial regression like Bayesian linear regression on top of the transformed inputs. Formally, the map that takes an input $\\mathbf{x}_n \\in \\mathbb{R}^{D'}$ and transforms it into a new input $\\phi(\\mathbf{x}_n) \\in \\mathbb{R}^{D}$ is called a **feature map**, $\\phi: \\mathbb{R}^{D'} \\to \\mathbb{R}^{D}$, for 1-dimensional input $x \\in \\mathbb{R}$, the polynomial feature map of degree $D$ is defined by\n",
        "\\begin{align}\n",
        "\\\\\\phi: \\mathbb{R} &\\to \\mathbb{R}^D\\\\\n",
        "x &\\mapsto [1, x, x^2, \\ldots, x^D]\\\\\n",
        "\\end{align}\n",
        "<br> Thus, we can write rewrite Bayesian polynomial regression as\n",
        "\\begin{align}\n",
        "\\\\y &= \\mathbf{w}^\\top \\mathbf{x} + \\epsilon, \\quad \\epsilon \\sim {N}(0, 0.3)\\\\\n",
        "\\mathbf{w} &\\sim {N}(0, \\alpha I_{D\\times D})\\\\\n",
        "\\end{align}\n",
        "<br>Denote the $N\\times D$ matrix of transformed inputs by $\\Phi$, where the $n$-th row of the matrix is the $n$-th input $\\mathbf{x}_n$ transformed by the feature map, $\\phi(\\mathbf{x}_n)$. Using this notation, write out the closed form for the posterior for Bayesian polynomial regression in terms of $\\Phi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCPKDQILI28x"
      },
      "source": [
        "2. **(Effect of Model Complexity)** For Bayesian kernel regression, you need to pre-determine the number of features (i.e. $D$) and the hyperparameter $\\alpha$ in the prior. For a toy dataset with 40 observations (set number_of_points=20), visualize the 95% posterior predictive interval for $D = [1,3,5,10,15,20,50,100]$ and $\\alpha = [0.1, 1, 5, 10, 100]$ (arrange these visualizations in a grid), using Bayesian polynomial regression.\n",
        "<br><br>\n",
        "Based on your visualizaion, describe in intuitive terms what is the role of $\\alpha$ and $D$ in determining the shape of the posterior predictive uncertainty.\n",
        "<br><br>\n",
        "***Hint:*** Read Problem 3 before implementing Problem 2, you can implement both at the same time.\n",
        "<br><br>\n",
        "**Extra Credit:** When the feature map $\\phi$ is a general (usually non-linear transformation), applying Bayesian linear regression on the transformed input is called **Bayesian Kernel Regression**. Choose your own non-linear feature map $\\phi: \\mathbb{R} \\to \\mathbb{R}^5$ and visualize the 95% posterior predictive interval of the Bayesian kernel regression for your choice of $\\phi$ and $D = [1,3,5,10,15,20,50,100]$, $\\alpha = [0.1, 1, 5, 10, 100]$. Compare the visualization to the that for Bayesian polynomial regression. Does the posterior predictive of your Bayesian kernel regression capture important properties of the posterior predictive of the Bayesian polynomial regression model?\n",
        "<br><br>\n",
        "**Note:** we highly recommend that you implement the following feature map:\n",
        "<br><br>\n",
        "\\begin{align}\n",
        "\\phi: \\mathbb{R}^{D'} &\\to \\mathbb{R}^D\\\\\n",
        "\\mathbf{x} &\\mapsto \\left[\\sqrt{\\frac{2}{D}} \\cos(w_1^\\top x + b_1), \\ldots, \\sqrt{\\frac{2}{D}} \\cos(w_D^\\top x + b_D)\\right]\n",
        "\\end{align}\n",
        "<br>where $b_d \\sim [0, 2\\pi]$ and $w_d \\sim N(0, \\beta I_{D'\\times D'})$ need to be randomly sampled and fixed before modeling and inference. For this exercise, we suggest setting $\\beta=10$. The features generated by $\\phi$ are called **Random Fourier Features**. As the number of features $D$ tends to infinity, the resulting Bayesian kernel regression model tends to an important type of Bayesian (nonparametric) model called Gaussian Process model. We will revisit the connection between Bayesian kernel regression and Gaussian processes in the latter part of the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0K7IXaJI28x"
      },
      "source": [
        "3. **(Model Evaluation and Uncertainty Estimation)** Remember that a direct visual comparision of the 95% predictive interval against the training data is impractical! Rather, to evaluate the fit of the Bayesian model on the observed data, we evaluate the marginal log-likelihood of the data under the posterior. Given a test set $\\{(\\mathbf{x}^*_m, \\mathbf{y}^*_m)\\}$, the log posterior predictive likelihood or, simply, the **log-likelihood** is computed as:\n",
        "\\begin{align}\n",
        "\\\\ \\log \\prod_{m=1}^M p(\\mathbf{y}^*_m | \\mathbf{x}^*_m, \\text{Data}) &= \\sum_{m=1}^M \\log p(\\mathbf{y}^*_m | \\mathbf{x}^*_m, \\text{Data})\\\\\n",
        "&= \\sum_{m=1}^M \\log \\int_\\mathbf{w} p(\\mathbf{y}^*_m | \\mathbf{x}^*_m, \\mathbf{w}) p(\\mathbf{w}| \\text{Data}) d\\mathbf{w}\n",
        "\\end{align}\n",
        "<br>i.e. the log-likelihood at a single observation $(\\mathbf{x}^*_m, \\mathbf{y}^*_m)$ is the log of the likelihood of the observation ***averaged over all models in the posterior***.\n",
        "<br><br>\n",
        "For Bayesian linear regression, with posterior ${N}(\\mu_N, \\Sigma_N)$ we have that\n",
        "$$\n",
        "p(y^*_m | x^*_m, \\text{Data}) = {N}(\\mu^\\top\\mathbf{x}^*_m, \\sigma^2 + (\\mathbf{x}^*_m)^\\top\\Sigma_N\\mathbf{x}^*_m)\n",
        "$$\n",
        "where $\\sigma^2$ is the variance of the observation noise.\n",
        "<br><br>\n",
        "For each choice of $D$ and $\\alpha$ in Problem 2, compute the log-likelihood of the training data. Examine a models with the higher log-likelihoods and a few with lower log-likelihoods, what is the relationship between log-likelihood and predictive uncertainty? In particular, does a higher log-likelihood indicate \"better\" predictive uncertainty?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCGIHR5I28x"
      },
      "source": [
        "2. **(Bayesian versus Frequentist Uncertainty)** Compare the types of predictive uncertainties that are generated by Bayesian models and ensembles. Characterize the advantages and disadvantages of bootstrap uncertainties from an ensemble. Describe an situation where it would be better to compute bootstrap uncertainties rather than posterior predictive uncertainties from a Bayesian model.\n",
        "\n",
        "  ***Hint:*** For example, consider situations where the data is scarce versus situations where the data is abundant; consider situations where clinicians can provide guidance on model selection using domain expertise versus situations where we would not know how patterns in the data would extrapolate to new populations of patients.\n",
        "\n",
        "  Characterize the advantages and disadvantages of posterior predictive uncertainties from a Bayesian model. Describe an application where it is better to use these uncertainties rather than bootstrap uncertainties from an ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IiJN8QQI28x"
      },
      "source": [
        "3. **(Measuring Uncertainty)** From your experiments, are any of the model evaluation metrics consdiered in this assignment (MSE, log-likelihood) appropriate for evaluating the quality of predictive uncertainty far away from the training data, that is, if we are concerned about the performance of models under covariate shift should we use these metrics to perform model selection?\n",
        "\n",
        "  Do our commonly used \"best practices\" of training machine learning models help or hamper our ability to train models with useful predictive uncertainties?\n",
        "\n",
        "  What would be a good metric for measuring uncertainty? How would you define \"good\" uncertainty in the first place?\n",
        "\n",
        "  ***Hint:*** Can you formulate a definition of \"good\" uncertainty without referencing a specific down-stream task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oaYSaFgMUNJ"
      },
      "source": [
        "## Part III: Calibration and Posterior Predictive Checks\n",
        "You may use these helpers as-is. Do not modify unless necessary for debugging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83240a60"
      },
      "outputs": [],
      "source": [
        "def reliability_diagram(probs, y_true, n_bins=10, title=None):\n",
        "    \"\"\"Plot a reliability diagram using confidence bins (binary case).\"\"\"\n",
        "    probs = np.asarray(probs)\n",
        "    y_true = np.asarray(y_true)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    bin_ids = np.digitize(probs, bins) - 1\n",
        "    bin_ids = np.clip(bin_ids, 0, n_bins - 1)\n",
        "\n",
        "    conf = np.zeros(n_bins)\n",
        "    acc = np.zeros(n_bins)\n",
        "    counts = np.zeros(n_bins)\n",
        "\n",
        "    for b in range(n_bins):\n",
        "        mask = bin_ids == b\n",
        "        counts[b] = mask.sum()\n",
        "        if counts[b] > 0:\n",
        "            conf[b] = probs[mask].mean()\n",
        "            acc[b] = y_true[mask].mean()\n",
        "        else:\n",
        "            conf[b] = np.nan\n",
        "            acc[b] = np.nan\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot([0, 1], [0, 1])\n",
        "    plt.scatter(conf, acc)\n",
        "    plt.xlabel('Mean predicted probability')\n",
        "    plt.ylabel('Empirical frequency')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "    return conf, acc, counts\n",
        "\n",
        "\n",
        "def ece_binary(probs, y_true, n_bins=10):\n",
        "    \"\"\"Expected Calibration Error (ECE) for binary classification.\"\"\"\n",
        "    probs = np.asarray(probs)\n",
        "    y_true = np.asarray(y_true)\n",
        "\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    bin_ids = np.digitize(probs, bins) - 1\n",
        "    bin_ids = np.clip(bin_ids, 0, n_bins - 1)\n",
        "\n",
        "    ece = 0.0\n",
        "    n = len(y_true)\n",
        "    for b in range(n_bins):\n",
        "        mask = bin_ids == b\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        acc = y_true[mask].mean()\n",
        "        conf = probs[mask].mean()\n",
        "        ece += (mask.sum() / n) * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "\n",
        "def brier_binary(probs, y_true):\n",
        "    probs = np.asarray(probs)\n",
        "    y_true = np.asarray(y_true)\n",
        "    return float(np.mean((probs - y_true)**2))\n",
        "\n",
        "\n",
        "def temperature_scale(logits, T):\n",
        "    \"\"\"Binary temperature scaling on logits.\"\"\"\n",
        "    logits = np.asarray(logits)\n",
        "    return 1 / (1 + np.exp(-logits / T))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sPxolYRNdry"
      },
      "source": [
        "\n",
        "First we will train a probabilistic classifier,\n",
        "evaluate NLL, Brier, ECE, and visualize calibration using a reliability diagram.\n",
        "\n",
        "**Dataset:** We generate a synthetic binary classification dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "c5c48c2d",
        "outputId": "9af5aeb6-28f7-4118-a533-65a596c2bb5d"
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(\n",
        "    n_samples=8000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=2,\n",
        "    flip_y=0.02,\n",
        "    class_sep=1.0,\n",
        "    random_state=0,\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "clf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "probs = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print('NLL (log loss):', log_loss(y_test, probs))\n",
        "print('Brier:', brier_binary(probs, y_test))\n",
        "print('ECE:', ece_binary(probs, y_test, n_bins=15))\n",
        "\n",
        "reliability_diagram(probs, y_test, n_bins=15, title='Reliability diagram (before calibration)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f338a6a"
      },
      "source": [
        "\n",
        "1. In the reliability diagram above, is the model **overconfident**, **underconfident**, or close to calibrated?\n",
        "2. Which metric among NLL, Brier, and ECE is *most directly* a calibration metric? Briefly justify.\n",
        "\n",
        "Write your answer in the markdown cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rpmZm8rPR0R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj8eaeb9PVOS"
      },
      "source": [
        "3. We will perform temperature scaling on a held-out calibration set.\n",
        "\n",
        "**Steps**\n",
        "Split X_train into a smaller train set and a calibration set. Fit the classifier on the smaller train set. Use the calibration set to choose a temperature T that minimizes NLL. Evaluate the reliability diagram, ECE, Brier, NLL on the test set before vs after scaling. What improved and what did not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "295aab53",
        "outputId": "62bf8e8a-00fd-424a-d3df-633cdade5df3"
      },
      "outputs": [],
      "source": [
        "# Split train into (train_small, cal)\n",
        "X_tr, X_cal, y_tr, y_cal = train_test_split(X_train, y_train, test_size=0.3, random_state=1, stratify=y_train)\n",
        "\n",
        "clf2 = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "clf2.fit(X_tr, y_tr)\n",
        "\n",
        "# Get logits for calibration and test\n",
        "logits_cal = clf2.named_steps['lr'].decision_function(clf2.named_steps['scaler'].transform(X_cal))\n",
        "logits_test = clf2.named_steps['lr'].decision_function(clf2.named_steps['scaler'].transform(X_test))\n",
        "\n",
        "# Grid search for T\n",
        "Ts = np.logspace(-2, 2, 80)\n",
        "cal_nll = []\n",
        "for T in Ts:\n",
        "    p = temperature_scale(logits_cal, T)\n",
        "    cal_nll.append(log_loss(y_cal, p))\n",
        "cal_nll = np.array(cal_nll)\n",
        "\n",
        "best_T = Ts[np.argmin(cal_nll)]\n",
        "print('Best T:', best_T)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Ts, cal_nll)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Temperature T')\n",
        "plt.ylabel('Calibration NLL')\n",
        "plt.title('Temperature scaling: choose T by minimizing NLL')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate before/after on test\n",
        "p_before = 1 / (1 + np.exp(-logits_test))\n",
        "p_after = temperature_scale(logits_test, best_T)\n",
        "\n",
        "print('--- Test metrics ---')\n",
        "print('Before: NLL', log_loss(y_test, p_before), 'Brier', brier_binary(p_before, y_test), 'ECE', ece_binary(p_before, y_test, n_bins=15))\n",
        "print('After : NLL', log_loss(y_test, p_after),  'Brier', brier_binary(p_after, y_test),  'ECE', ece_binary(p_after, y_test, n_bins=15))\n",
        "\n",
        "reliability_diagram(p_before, y_test, n_bins=15, title='Reliability diagram (before temp scaling)')\n",
        "reliability_diagram(p_after, y_test, n_bins=15, title='Reliability diagram (after temp scaling)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38975647"
      },
      "source": [
        "\n",
        "### **Posterior Predictive Checks (PPCs) for regression**\n",
        "\n",
        "We fit a simple probabilistic regression model and run PPC-style checks.\n",
        "\n",
        "We assume Gaussian noise:\n",
        "\\[\n",
        "Y \\mid x \\sim N(f(x), \\sigma^2).\n",
        "\\]\n",
        "\n",
        "We will:\n",
        "- fit a ridge regression model for the mean function, and\n",
        "- estimate \\(\\sigma\\) on the training set,\n",
        "then use this as a predictive distribution.\n",
        "\n",
        "This is a simplified proxy to practice PPC mechanics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b93e837",
        "outputId": "cdff6e4b-9f49-46cc-89c3-3353d2e265e6"
      },
      "outputs": [],
      "source": [
        "Xr, yr = make_regression(n_samples=4000, n_features=10, noise=15.0, random_state=0)\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.3, random_state=0)\n",
        "\n",
        "reg = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('ridge', Ridge(alpha=1.0))\n",
        "])\n",
        "reg.fit(Xr_train, yr_train)\n",
        "\n",
        "mu_train = reg.predict(Xr_train)\n",
        "mu_test = reg.predict(Xr_test)\n",
        "\n",
        "# Estimate sigma from training residuals\n",
        "resid = yr_train - mu_train\n",
        "sigma_hat = np.std(resid)\n",
        "print('Estimated sigma:', sigma_hat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6D86TNxRHea"
      },
      "source": [
        "4. Implement the average predictive negative log likelihood for a Gaussian predictive distribution: $ -\\frac{1}{n}\\sum_i \\log N(y_i \\mid \\mu_i, \\sigma^2). $ Return a scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b14917b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def gaussian_nll(y, mu, sigma):\n",
        "    # TODO: implement average negative log likelihood for N(mu, sigma^2)\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Uncomment after implementing\n",
        "# print('Test NLL:', gaussian_nll(yr_test, mu_test, sigma_hat))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7YwbgMYRvHq"
      },
      "source": [
        "5. Perform a basic PPC: Simulate replicated outcomes $\\tilde {y}^{(s)}\\sim {N}(\\mu_{test}, \\sigma^2)$ for $S = 500$ replicates and compare statistics $T$ between replicated and observed outcomes. Use two statistics: $T_1$ = mean($y$); $T_2$ = variance($y$). Provide a plot showing the distribution of replicated $T_1$ and $T_2$ with the observed value marked and a short interpretation of whether the model appears to fit these statistics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epMl5bShQ4e8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQbb3RITJ0o"
      },
      "source": [
        "# Part IV: Conformal Prediction\n",
        "We will implement split conformal prediction for regression.\n",
        "\n",
        "Goal: produce prediction intervals $[L(x), U(x)]$ with approximate marginal coverage $(1-\\alpha)$.\n",
        "\n",
        "**Split conformal prediction for regression:**\n",
        "\n",
        "i) Fit model on training set.\n",
        "\n",
        "ii) On calibration set, compute residuals $r_i = |y_i - \\hat{y}_i|$.\n",
        "\n",
        "iii) Let $q$ be the $(1-\\alpha)$-quantile of ${r_i}$.\n",
        "Output interval for a new point $x$: $ [\\hat{y}(x)-q,\\ \\hat{y}(x)+q]. $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fcd26df"
      },
      "outputs": [],
      "source": [
        "# Prepare data splits\n",
        "Xr_tr, Xr_temp, yr_tr, yr_temp = train_test_split(Xr, yr, test_size=0.4, random_state=0)\n",
        "Xr_cal, Xr_te, yr_cal, yr_te = train_test_split(Xr_temp, yr_temp, test_size=0.5, random_state=0)\n",
        "\n",
        "base = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('ridge', Ridge(alpha=1.0))\n",
        "])\n",
        "base.fit(Xr_tr, yr_tr)\n",
        "\n",
        "pred_cal = base.predict(Xr_cal)\n",
        "pred_te = base.predict(Xr_te)\n",
        "\n",
        "resid_cal = np.abs(yr_cal - pred_cal)\n",
        "\n",
        "def split_conformal_interval(pred, resid_cal, alpha=0.1):\n",
        "    # TODO: compute q = (1-alpha)-quantile of calibration residuals\n",
        "    # Return lower, upper arrays for pred\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Uncomment after implementing\n",
        "# L, U = split_conformal_interval(pred_te, resid_cal, alpha=0.1)\n",
        "# coverage = np.mean((yr_te >= L) & (yr_te <= U))\n",
        "# avg_width = np.mean(U - L)\n",
        "# print('Empirical coverage:', coverage)\n",
        "# print('Average interval width:', avg_width)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "198f0f94"
      },
      "source": [
        "1. After implementing `split_conformal_interval`:\n",
        "\n",
        "i) Compute empirical coverage for $\\alpha\\in\\{0.05, 0.1, 0.2\\}$.\n",
        "\n",
        "ii) Report average width for each $\\alpha$.\n",
        "\n",
        "iii) Briefly interpret the tradeoff.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn74q87lUqmR"
      },
      "source": [
        "2. In 5–8 sentences:\n",
        "\n",
        "Under what assumptions does split conformal guarantee coverage?\n",
        "\n",
        "Why might those assumptions fail under covariate shift?\n",
        "\n",
        "What is the high-level idea behind weighted conformal methods?\n",
        "\n",
        "Write your answer below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADHWMiHUUboa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
